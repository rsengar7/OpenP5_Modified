{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6ae3f02-1087-4bf5-af62-5befabd97bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.27.3\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "50918e01-fa45-44e4-bac9-4bc70111eacd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[112], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00567024-ae15-4cec-8107-46cbde83748e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sequential', 'direct', 'straightforward']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'sequential,direct,straightforward'\n",
    "a = a.split(',')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75757b6b-9573-4e4c-a891-c7b27843dd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the 10\n"
     ]
    }
   ],
   "source": [
    "template = 'This is the {value}'\n",
    "context = {'value': 10}\n",
    "print(template.format(**context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da84a97e-ce55-4d62-a9fb-c02b25deb673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadLineFromFile(path):\n",
    "    lines = []\n",
    "    with open(path,'r') as fd:\n",
    "        for line in fd:\n",
    "            lines.append(line.rstrip('\\n'))\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "180dc3b4-5d47-4ae3-8579-f5c68e1f6ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './prompt_template.txt'\n",
    "data = ReadLineFromFile(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6974fddf-1e95-4172-92fc-acb41072b374",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = dict()\n",
    "for prompt in data:\n",
    "    t = [sens.strip() for sens in prompt.split(';')]\n",
    "    if t[0] not in prompt_template:\n",
    "        prompt_template[t[0]] = dict()\n",
    "    if t[1] not in prompt_template[t[0]]:\n",
    "        prompt_template[t[0]][t[1]] = dict()\n",
    "    num = len(prompt_template[t[0]][t[1]])\n",
    "    prompt_template[t[0]][t[1]][str(num)] = dict()\n",
    "    prompt_template[t[0]][t[1]][str(num)]['Input'] = t[2]\n",
    "    prompt_template[t[0]][t[1]][str(num)]['Output'] = t[3]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab2e1b82-b66a-46e4-afb8-8a70de111a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequential': {'seen': {'0': {'Input': 'Considering {dataset} user {user_id} has interacted with {dataset} items {history}. What is the next recommendation for the user?',\n",
       "    'Output': '{dataset} item {target}'},\n",
       "   '1': {'Input': 'Here is the purchase history of {dataset} user {user_id}: {dataset} item {history}. I wonder what is the next recommended item for the user.',\n",
       "    'Output': '{dataset} item {target}'}},\n",
       "  'unseen': {'0': {'Input': 'What is the top recommended item for {dataset} user {user_id} who interacted with {dataset} item {history}?',\n",
       "    'Output': '{dataset} item {target}'}}},\n",
       " 'straightforward': {'seen': {'0': {'Input': 'What should we recommend for {dataset} user {user_id}?',\n",
       "    'Output': '{dataset} item {target}'},\n",
       "   '1': {'Input': '{dataset} user {user_id} is looking for some items. Do you have any recommendations?',\n",
       "    'Output': '{dataset} item {target}'}},\n",
       "  'unseen': {'0': {'Input': 'What is the top recommendation for {dataset} user {user_id}?',\n",
       "    'Output': '{dataset} item {target}'}}}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e4a87c2-882d-4084-b915-4150102ac9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapoint = {'dataset':'amazon', 'user_id': 11, 'history':'2,3,4,5', 'target': 23}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd698da1-3125-4561-954b-3763b31a5dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Considering amazon user 11 has interacted with amazon items 2,3,4,5. What is the next recommendation for the user?'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template['sequential']['seen']['0']['Input'].format(**datapoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14663bc3-036c-45d1-9915-4e3d5ff12155",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "No prompt for review task",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m prompt_template \u001b[38;5;241m=\u001b[39m load_prompt_template(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../prompt_template.txt\u001b[39m\u001b[38;5;124m'\u001b[39m,task_list)\n\u001b[1;32m      4\u001b[0m prompt_info \u001b[38;5;241m=\u001b[39m get_info_from_prompt(prompt_template)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mcheck_task_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_list\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/SP5/src/utils/prompt.py:60\u001b[0m, in \u001b[0;36mcheck_task_prompt\u001b[0;34m(prompt_templates, task_list)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03mCheck if all tasks have prompt templates. Raise Error if training tasks have no prompt.\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03mInput:\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m- prompt_templates: A dictionary of prompt templates.\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03m- task_list: A list of training tasks.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m task_list:\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m prompt_templates, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo prompt for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m task\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: No prompt for review task"
     ]
    }
   ],
   "source": [
    "from utils.prompt import load_prompt_template, get_info_from_prompt, check_task_prompt\n",
    "task_list = ['sequential', 'straightforward', 'direct', 'review']\n",
    "prompt_template = load_prompt_template('../prompt_template.txt',task_list)\n",
    "prompt_info = get_info_from_prompt(prompt_template)\n",
    "check_task_prompt(prompt_template, task_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb153e99-70ee-4355-bf0e-07cc54a8585b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequential': {'seen': {'0': {'Input': 'Considering {dataset} user {user_id} has interacted with {dataset} items {history}. What is the next recommendation for the user?',\n",
       "    'Output': '{dataset} item {target}'},\n",
       "   '1': {'Input': 'Here is the purchase history of {dataset} user {user_id}: {dataset} item {history}. I wonder what is the next recommended item for the user.',\n",
       "    'Output': '{dataset} item {target}'}},\n",
       "  'unseen': {'0': {'Input': 'What is the top recommended item for {dataset} user {user_id} who interacted with {dataset} item {history}?',\n",
       "    'Output': '{dataset} item {target}'}}},\n",
       " 'direct': {'seen': {'0': {'Input': 'Choose the best item from the candidates to recommend for {dataset} user {user_id}: {dataset} items {candidate_items}',\n",
       "    'Output': '{dataset} item {target}'},\n",
       "   '1': {'Input': 'Pick the most suitable item from the following list and recommend to {dataset} user {user_id}: {dataset} items {candidate_items}',\n",
       "    'Output': '{dataset} item {target}'}},\n",
       "  'unseen': {'0': {'Input': 'We want to make recommendation for {dataset} user {user_id}. Select the best item from these candidates: {dataset} items {candidate_items}',\n",
       "    'Output': '{dataset} item {target}'}}},\n",
       " 'straightforward': {'seen': {'0': {'Input': 'What should we recommend for {dataset} user {user_id}?',\n",
       "    'Output': '{dataset} item {target}'},\n",
       "   '1': {'Input': '{dataset} user {user_id} is looking for some items. Do you have any recommendations?',\n",
       "    'Output': '{dataset} item {target}'}},\n",
       "  'unseen': {'0': {'Input': 'What is the top recommendation for {dataset} user {user_id}?',\n",
       "    'Output': '{dataset} item {target}'}}}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3848a5d5-8bd7-4360-a753-762c19dd7f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = prompt_template['sequential']['seen']['0']['Input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c38c5328-98f0-432b-8de6-f80e47c07b00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{dataset}', '{user_id}', '{dataset}', '{history}']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.findall(r'\\{.*?\\}', inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbae0b3c-4b13-4911-ad66-8dcc5bc0776c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user_id', 'history', 'target', 'dataset', 'candidate_items']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info = []\n",
    "for task in prompt_template:\n",
    "    for see in prompt_template[task]:\n",
    "        for i in prompt_template[task][see]:\n",
    "            info += re.findall(r'\\{.*?\\}', prompt_template[task][see][i]['Input'])\n",
    "            info += re.findall(r'\\{.*?\\}', prompt_template[task][see][i]['Output'])\n",
    "info = [i[1:-1] for i in set(info)]\n",
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "25aa7ef8-0ad1-4f4f-99df-0eda1810e948",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class MyFirstDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        # dummy dataset\n",
    "        self.samples = [i+1 for i in range(30)]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # change this to your samples fetching logic\n",
    "        return self.samples[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        # change this to return number of samples in your dataset\n",
    "        return len(self.samples)\n",
    "    \n",
    "class MySecondDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        # dummy dataset\n",
    "        self.samples = [i+1 for i in range(30,90)]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # change this to your samples fetching logic\n",
    "        return self.samples[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        # change this to return number of samples in your dataset\n",
    "        return len(self.samples)\n",
    "    \n",
    "fd = MyFirstDataset()\n",
    "sd = MySecondDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2209bf24-c47f-4f6c-917e-8879dcda32ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import ConcatDataset\n",
    "cd = ConcatDataset([fd, sd])\n",
    "for i in range(len(cd)):\n",
    "    print(cd[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c1aac4c-2f6a-4aab-adfa-a83e49703cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/home/sx86/anaconda3/envs/SP5/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb7fa571-31ea-40b9-a95d-ea78ead1118d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 11, 10, 9, 6, 1, 7, 8, 3, 5, 2]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "a = dict()\n",
    "a['1'] = [1,2,3,4,5]\n",
    "a['2'] = [6,7,8]\n",
    "a['3'] = [1,2,3,9,10,11]\n",
    "items = set()\n",
    "for i in a:\n",
    "    items.update(a[i])\n",
    "items = list(items)\n",
    "random.shuffle(items)\n",
    "items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e6ec461-e394-449f-b9fb-a01867905f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('1', [1, 2, 3, 4, 5]), ('2', [6, 7, 8]), ('3', [1, 2, 3, 9, 10, 11])])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31aaaee1-e144-4103-89e3-9627ed65e3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_list = sorted(a, key=lambda x: len(a[x]), reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a0e7dcc-a74e-4891-8ed4-95014604e4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 7, 8]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 9, 10, 11]\n"
     ]
    }
   ],
   "source": [
    "for u in user_list:\n",
    "    print(a[u])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fd90c844-1583-4f67-aa42-71c886bcb3d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m a \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m20\u001b[39m)]\n\u001b[1;32m      5\u001b[0m indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandperm(\u001b[38;5;28mlen\u001b[39m(a), generator\u001b[38;5;241m=\u001b[39mg)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m----> 6\u001b[0m \u001b[43ma\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not list"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "g = torch.Generator()\n",
    "g.manual_seed(2018)\n",
    "a = [i for i in range(10,20)]\n",
    "indices = torch.randperm(len(a), generator=g).tolist()\n",
    "a[[indices]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "053e5dfe-76c4-4c3c-b422-c92d6a00e08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class MyFirstDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        # dummy dataset\n",
    "        self.samples = [i for i in range(30)]\n",
    "        self.task_data = dict()\n",
    "        self.task_data['1'] = [i for i in range(10)]\n",
    "        self.task_data['2'] = [i for i in range(10,20)]\n",
    "        self.task_data['3'] = [i for i in range(20,30)]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # change this to your samples fetching logic\n",
    "        return self.samples[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        # change this to return number of samples in your dataset\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def shuffle(self, seed):\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(seed)\n",
    "        for task in self.task_data:\n",
    "            indices = torch.randperm(len(self.task_data[task]), generator=g).tolist()\n",
    "            self.task_data[task] = [self.task_data[task][i] for i in indices]\n",
    "    \n",
    "class MySecondDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        # dummy dataset\n",
    "        self.samples = [i for i in range(30,90)]\n",
    "        self.task_data = dict()\n",
    "        self.task_data['1'] = [i for i in range(20)]\n",
    "        self.task_data['2'] = [i for i in range(20,40)]\n",
    "        self.task_data['3'] = [i for i in range(40,60)]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # change this to your samples fetching logic\n",
    "        return self.samples[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        # change this to return number of samples in your dataset\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def shuffle(self, seed):\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(seed)\n",
    "        \n",
    "        for task in self.task_data:\n",
    "            indices = torch.randperm(len(self.task_data[task]), generator=g).tolist()\n",
    "            self.task_data[task] = [self.task_data[task][i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4612df1-7c9a-4083-bda5-04596a436135",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import Sampler, RandomSampler, SequentialSampler\n",
    "import math\n",
    "import torch\n",
    "class MySampler(Sampler):\n",
    "    def __init__(self, data_source, batch_size):\n",
    "        self.data_source = data_source\n",
    "        self.batch_size = batch_size\n",
    "        self.dataset_task_size = []\n",
    "        for ds in self.data_source.datasets:\n",
    "            for task in ds.task_data:\n",
    "                self.dataset_task_size.append(len(ds.task_data[task]))\n",
    "        self.largest_task_size = max(self.dataset_task_size)\n",
    "    def set_epoch(self, epoch):\n",
    "        self.epoch = epoch\n",
    "    def __iter__(self):\n",
    "        sampler_list = []\n",
    "        iterator_list = []\n",
    "        for i in range(len(self.data_source.datasets)):\n",
    "            ds = self.data_source.datasets[i]\n",
    "            ds.shuffle(self.epoch)\n",
    "            for task in ds.task_data:\n",
    "                sampler = SequentialSampler(ds.task_data[task])\n",
    "                sampler_list.append(sampler)\n",
    "                iterator = sampler.__iter__()\n",
    "                iterator_list.append(iterator)\n",
    "        \n",
    "        cum_index = [0] + self.data_source.cumulative_sizes[:-1]\n",
    "        task_cum_index = []\n",
    "        for i in range(len(self.data_source.datasets)):\n",
    "            ds = self.data_source.datasets[i]\n",
    "            cur_cum_index = cum_index[i]\n",
    "            for task in ds.task_data:\n",
    "                task_cum_index.append(cur_cum_index)\n",
    "                cur_cum_index += len(ds.task_data[task])\n",
    "                \n",
    "                \n",
    "        step = self.batch_size * len(self.dataset_task_size)\n",
    "        epoch_data_size = self.largest_task_size * len(self.dataset_task_size)\n",
    "        \n",
    "        final_list = []\n",
    "        for _ in range(0, epoch_data_size, step):\n",
    "            for i in range(len(self.dataset_task_size)):\n",
    "                cur_iterator = iterator_list[i]\n",
    "                cur_samples = []\n",
    "                for _ in range(self.batch_size):\n",
    "                    try:\n",
    "                        cur_element = cur_iterator.__next__()\n",
    "                        cur_element += task_cum_index[i]\n",
    "                        cur_samples.append(cur_element)\n",
    "                        \n",
    "                    except StopIteration:\n",
    "                        iterator_list[i] = sampler_list[i].__iter__()\n",
    "                        cur_iterator = iterator_list[i]\n",
    "                        cur_element = cur_iterator.__next__()\n",
    "                        cur_element += task_cum_index[i]\n",
    "                        cur_samples.append(cur_element)\n",
    "                final_list.extend(cur_samples)\n",
    "        print(final_list)\n",
    "        return iter(final_list)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.batch_size * math.ceil(self.largest_task_size / self.batch_size) * len(self.dataset_task_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0f0f097-d70d-4eee-afd1-6d02c2b2fe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.distributed import DistributedSampler\n",
    "class DistMultiDataTaskSampler(DistributedSampler):\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        super().__init__(dataset)\n",
    "        self.batch_size = batch_size\n",
    "        self.dataset_task_size = []\n",
    "        for ds in self.dataset.datasets:\n",
    "            for task in ds.task_data:\n",
    "                self.dataset_task_size.append(len(ds.task_data[task]))\n",
    "        self.largest_task_size = max(self.dataset_task_size)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        sampler_list = []\n",
    "        iterator_list = []\n",
    "        for i in range(len(self.dataset.datasets)):\n",
    "            ds = self.dataset.datasets[i]\n",
    "            for task in ds.task_data:\n",
    "                sampler = DistributedSampler(ds.task_data[task])\n",
    "                sampler_list.append(sampler)\n",
    "                iterator = sampler.__iter__()\n",
    "                iterator_list.append(iterator)\n",
    "        \n",
    "        cum_index = [0] + self.dataset.cumulative_sizes[:-1]\n",
    "        task_cum_index = []\n",
    "        for i in range(len(self.dataset.datasets)):\n",
    "            ds = self.dataset.datasets[i]\n",
    "            cur_cum_index = cum_index[i]\n",
    "            for task in ds.task_data:\n",
    "                task_cum_index.append(cur_cum_index)\n",
    "                cur_cum_index += len(ds.task_data[task])\n",
    "                \n",
    "                \n",
    "        step = self.batch_size * len(self.dataset_task_size)\n",
    "        epoch_data_size = self.largest_task_size * len(self.dataset_task_size)\n",
    "        \n",
    "        final_list = []\n",
    "        for _ in range(0, epoch_data_size, step):\n",
    "            for i in range(len(self.dataset_task_size)):\n",
    "                cur_iterator = iterator_list[i]\n",
    "                cur_samples = []\n",
    "                for _ in range(self.batch_size):\n",
    "                    try:\n",
    "                        cur_element = cur_iterator.__next__()\n",
    "                        cur_element += task_cum_index[i]\n",
    "                        cur_samples.append(cur_element)\n",
    "                        \n",
    "                    except StopIteration:\n",
    "                        iterator_list[i] = sampler_list[i].__iter__()\n",
    "                        cur_iterator = iterator_list[i]\n",
    "                        cur_element = cur_iterator.__next__()\n",
    "                        cur_element += task_cum_index[i]\n",
    "                        cur_samples.append(cur_element)\n",
    "                final_list.extend(cur_samples)\n",
    "        \n",
    "        return iter(final_list)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.batch_size * math.ceil(self.largest_task_size / self.batch_size) * len(self.dataset_task_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a8622baf-72c7-43e8-abec-c6b050b1d128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/common/home/sx86/anaconda3/envs/SP5/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/common/home/sx86/anaconda3/envs/SP5/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'main' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "ProcessExitedException",
     "evalue": "process 1 terminated with exit code 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProcessExitedException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mngpus_per_node\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     21\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_VISIBLE_DEVICES\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0,1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 22\u001b[0m \u001b[43mmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mngpus_per_node\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/SP5/lib/python3.9/site-packages/torch/multiprocessing/spawn.py:240\u001b[0m, in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    236\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis method only supports start_method=spawn (got: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    237\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTo use a different start_method use:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    238\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m torch.multiprocessing.start_processes(...)\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m start_method)\n\u001b[1;32m    239\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(msg)\n\u001b[0;32m--> 240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstart_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdaemon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspawn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/SP5/lib/python3.9/site-packages/torch/multiprocessing/spawn.py:198\u001b[0m, in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# Loop on join until it returns True or raises an exception.\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/SP5/lib/python3.9/site-packages/torch/multiprocessing/spawn.py:149\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ProcessExitedException(\n\u001b[1;32m    141\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocess \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m terminated with signal \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    142\u001b[0m             (error_index, name),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    146\u001b[0m             signal_name\u001b[38;5;241m=\u001b[39mname\n\u001b[1;32m    147\u001b[0m         )\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 149\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ProcessExitedException(\n\u001b[1;32m    150\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocess \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m terminated with exit code \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    151\u001b[0m             (error_index, exitcode),\n\u001b[1;32m    152\u001b[0m             error_index\u001b[38;5;241m=\u001b[39merror_index,\n\u001b[1;32m    153\u001b[0m             error_pid\u001b[38;5;241m=\u001b[39mfailed_process\u001b[38;5;241m.\u001b[39mpid,\n\u001b[1;32m    154\u001b[0m             exit_code\u001b[38;5;241m=\u001b[39mexitcode\n\u001b[1;32m    155\u001b[0m         )\n\u001b[1;32m    157\u001b[0m original_trace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_queues[error_index]\u001b[38;5;241m.\u001b[39mget()\n\u001b[1;32m    158\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-- Process \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m terminated with the following error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m error_index\n",
      "\u001b[0;31mProcessExitedException\u001b[0m: process 1 terminated with exit code 1"
     ]
    }
   ],
   "source": [
    "import torch.multiprocessing as mp\n",
    "import os\n",
    "from torch.utils.data.dataset import ConcatDataset\n",
    "def main(rank, cd):\n",
    "    fd = MyFirstDataset()\n",
    "    sd = MySecondDataset()\n",
    "    cd = ConcatDataset([fd, sd])\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    dist.init_process_group(\n",
    "        backend=\"nccl\", world_size=2, rank=local_rank\n",
    "    )\n",
    "    dataloader = DataLoader(dataset=cd, sampler=DistMultiDataTaskSampler(cd,args['bs']), batch_size=['bs'], shuffle=False)\n",
    "    for data in dataloader:\n",
    "        print(f\"rank {rank}: {data}\")\n",
    "\n",
    "args = dict()\n",
    "args['bs'] = 5\n",
    "args['ngpus_per_node'] = 2\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0,1'\n",
    "mp.spawn(\n",
    "        main, args=(args,), nprocs=args['ngpus_per_node'], join=True\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa9ea375-c159-4e99-8ad0-0c5f31c9ee1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "[0, 1, 2, 3, 4, 10, 11, 12, 13, 14, 20, 21, 22, 23, 24, 30, 31, 32, 33, 34, 50, 51, 52, 53, 54, 70, 71, 72, 73, 74, 5, 6, 7, 8, 9, 15, 16, 17, 18, 19, 25, 26, 27, 28, 29, 35, 36, 37, 38, 39, 55, 56, 57, 58, 59, 75, 76, 77, 78, 79, 0, 1, 2, 3, 4, 10, 11, 12, 13, 14, 20, 21, 22, 23, 24, 40, 41, 42, 43, 44, 60, 61, 62, 63, 64, 80, 81, 82, 83, 84, 5, 6, 7, 8, 9, 15, 16, 17, 18, 19, 25, 26, 27, 28, 29, 45, 46, 47, 48, 49, 65, 66, 67, 68, 69, 85, 86, 87, 88, 89]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "fd = MyFirstDataset()\n",
    "sd = MySecondDataset()\n",
    "cd = ConcatDataset([fd, sd])\n",
    "bs = 5\n",
    "dataloader = DataLoader(dataset=cd, sampler=MySampler(cd,bs), batch_size=bs, shuffle=False)\n",
    "print(len(dataloader))\n",
    "dataloader.sampler.set_epoch(0)\n",
    "for data in dataloader:\n",
    "    continue\n",
    "# for i in range(3):\n",
    "#     print(f\"epoch {i}\")\n",
    "#     dataloader.sampler.set_epoch(i)\n",
    "#     for data in dataloader:\n",
    "#         print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "79a2ff5e-985f-4908-b58b-0ade641742b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 5, 0, 6, 2, 3, 9, 4, 1, 7]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader.dataset.datasets[0].task_data['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a85d99fa-7068-4cc3-964e-751ba5600048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 6, 7, 9, 2, 11, 18, 19, 17, 14, 26, 25, 20, 22, 23, 34, 39, 36, 31, 32, 40, 46, 45, 43, 44, 52, 54, 51, 58, 50, 1, 8, 5, 4, 0, 16, 12, 13, 10, 15, 28, 24, 27, 29, 21, 33, 35, 38, 30, 37, 48, 42, 47, 41, 49, 56, 53, 55, 57, 59]\n",
      "60\n",
      "tensor([3, 6, 7, 9, 2])\n",
      "tensor([11, 18, 19, 17, 14])\n",
      "tensor([26, 25, 20, 22, 23])\n",
      "tensor([34, 39, 36, 31, 32])\n",
      "tensor([40, 46, 45, 43, 44])\n",
      "tensor([52, 54, 51, 58, 50])\n",
      "tensor([1, 8, 5, 4, 0])\n",
      "tensor([16, 12, 13, 10, 15])\n",
      "tensor([28, 24, 27, 29, 21])\n",
      "tensor([33, 35, 38, 30, 37])\n",
      "tensor([48, 42, 47, 41, 49])\n",
      "tensor([56, 53, 55, 57, 59])\n"
     ]
    }
   ],
   "source": [
    "for data in dataloader:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "048bb939-6576-4996-8e78-1e8752044b04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 12, 13, 14, 15, 16, 17, 18, 19]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in range(11,20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3a7b31af-4f7e-4449-86f5-2612555c4669",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter([1,2,3,4,5,6,7,8,9,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a5bdfb74-ecf4-40d1-af52-15c047319512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterator.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "08a7014a-9020-44ec-a745-7147026b6f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = SequentialSampler([i for i in range(10,20)])\n",
    "iterator = sampler.__iter__()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b0ced435-2c2b-4d08-b655-b22da0951635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterator.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "05e52ae5-92ce-4719-953d-dbda9d48817e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[]\n",
      "[]\n",
      "2\n",
      "[1]\n",
      "[1]\n",
      "3\n",
      "[1, 2]\n",
      "[1, 2]\n",
      "4\n",
      "[1, 2, 3]\n",
      "[1, 2, 3]\n",
      "5\n",
      "[1, 2, 3, 4]\n",
      "[2, 3, 4]\n",
      "6\n",
      "[1, 2, 3, 4, 5]\n",
      "[3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "a = [1,2,3,4,5,6]\n",
    "for i in range(len(a)):\n",
    "    print(a[i])\n",
    "    print(a[:i])\n",
    "    print(a[:i][-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "77970cdb-ca6d-4f77-9467-d32220be1337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:-2][-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "379645ad-1dc4-4243-b25f-d989871811b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {'a':'1', 'b':'2', 'c':'3'}\n",
    "list(a.values())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "62c139e7-eb59-459c-8f8a-88e370952034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.randint(0,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fcbdb4ca-8319-420c-96c3-112eda060cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 1, 5, 4, 2]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3,4,5]\n",
    "random.shuffle(a)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "789491f3-475c-451b-bbcd-c96c22c7f85a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'a': '3', 'b': '2'}, {'a': '3', 'b': '2'}, {'a': '3', 'b': '2'}]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_list = []\n",
    "for i in range(3):\n",
    "    a = dict()\n",
    "    a['a'] = '1'\n",
    "    a['b'] = '2'\n",
    "    all_list.append(a)\n",
    "for a in all_list:\n",
    "    a['a'] = '3'\n",
    "all_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d6342f66-c550-46d1-a405-61cf50dccc36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = [2]\n",
    "idx = [num[0] * 10]\n",
    "for i in range(1, len(num)):\n",
    "    idx.append(num[i] * 10 + idx[i-1])\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "bc4d79cd-9773-4c76-9c31-a5764281bd7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "51c87925-2d7d-40e0-b4c0-32b0bb3c9d96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['seen', '1']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'seen:1'.split(':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2e7e8814-e7d1-41d7-8762-1a985115c693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': [1, 3], 'b': [2]}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = dict()\n",
    "a['a'] = [1]\n",
    "a['b'] = [2]\n",
    "a['a'].append(3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "03b95048-eb42-40e3-857f-4724b7582aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "if a:\n",
    "    print('a is true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "cad3178b-0956-4b59-aa82-9050d168c073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in range(10,20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d85d0702-a1a0-430f-97f7-ab9a74d12b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "a = False\n",
    "b = 1 if a else 0\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d0dd497a-6dcc-44fb-8082-e73c6631c688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'t5' in 't5-small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "195aee63-b947-4937-806a-9888b0f3e02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/home/sx86/anaconda3/envs/SP5/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/common/home/sx86/anaconda3/envs/SP5/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f47917b-c3fd-414b-90dd-9d8c0c21596c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Input_text = 'Here is the purchase history of Beauty user 1: Beauty item 1,2,3,4,5. I wonder what is the next recommended item for the user.'\n",
    "force_word = ['Beauty']\n",
    "input_ids = tokenizer(Input_text,return_tensors=\"pt\").input_ids\n",
    "force_word_ids = tokenizer(force_word).input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22cd9c03-9478-4194-87c3-f0ee9edacc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = [\n",
    "        \"3560\",\n",
    "        \"554\",\n",
    "        \"1825\",\n",
    "        \"1062\",\n",
    "        \"680\",\n",
    "        \"1683\",\n",
    "        \"363\",\n",
    "        \"927\",\n",
    "        \"2345\",\n",
    "        \"1398\",\n",
    "        \"2000\",\n",
    "        \"599\",\n",
    "        \"375\",\n",
    "        \"3637\",\n",
    "        \"3272\",\n",
    "        \"153\",\n",
    "    ]\n",
    "force_words_ids = [\n",
    "    tokenizer(force_word, add_special_tokens=False).input_ids,\n",
    "    tokenizer(candidates, add_special_tokens=False).input_ids,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f768d3e4-dbe5-4ece-8a02-0651bddb399f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[292]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "force_words = [\"Sie\"]\n",
    "force_words_ids = tokenizer(force_words, add_special_tokens=False).input_ids\n",
    "force_words_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d134b999-b072-4c40-b6b1-d9171556ce56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beauty item 1,2,3,4,5\n"
     ]
    }
   ],
   "source": [
    "prediction = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    force_words_ids=force_words_ids,\n",
    "    max_length=8,\n",
    "    num_beams=5,\n",
    "    num_return_sequences=5,\n",
    ")\n",
    "print(tokenizer.decode(prediction[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "417dc3d0-d091-40b4-9584-39af2c0566b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"</s>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d437089c-849e-46a6-9433-58fd8e873349",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_words_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_word_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_invalid_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "File \u001b[0;32m~/anaconda3/envs/SP5/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/SP5/lib/python3.9/site-packages/transformers/generation/utils.py:1239\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, **kwargs)\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;66;03m# 3. Define model inputs\u001b[39;00m\n\u001b[1;32m   1232\u001b[0m \u001b[38;5;66;03m# inputs_tensor has to be defined\u001b[39;00m\n\u001b[1;32m   1233\u001b[0m \u001b[38;5;66;03m# model_input_name is defined if model-specific keyword input is passed\u001b[39;00m\n\u001b[1;32m   1234\u001b[0m \u001b[38;5;66;03m# otherwise model_input_name is None\u001b[39;00m\n\u001b[1;32m   1235\u001b[0m \u001b[38;5;66;03m# all model-specific keyword inputs are removed from `model_kwargs`\u001b[39;00m\n\u001b[1;32m   1236\u001b[0m inputs_tensor, model_input_name, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_model_inputs(\n\u001b[1;32m   1237\u001b[0m     inputs, generation_config\u001b[38;5;241m.\u001b[39mbos_token_id, model_kwargs\n\u001b[1;32m   1238\u001b[0m )\n\u001b[0;32m-> 1239\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[43minputs_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1241\u001b[0m \u001b[38;5;66;03m# 4. Define other model kwargs\u001b[39;00m\n\u001b[1;32m   1242\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_attentions\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39moutput_attentions\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    force_words_ids=force_word_ids,\n",
    "    max_length=8,\n",
    "    num_beams=5,\n",
    "    num_return_sequences=1,\n",
    "    no_repeat_ngram_size=1,\n",
    "    remove_invalid_values=True,\n",
    ")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1d354103-d75c-4658-a926-37e3659cd8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.generation_trie import Trie\n",
    "candidates = [\n",
    "        \"3560\",\n",
    "        \"554\",\n",
    "        \"1825\",\n",
    "        \"1062\",\n",
    "        \"680\",\n",
    "        \"1683\",\n",
    "        \"363\",\n",
    "        \"927\",\n",
    "        \"2345\",\n",
    "        \"1398\",\n",
    "        \"2000\",\n",
    "        \"599\",\n",
    "        \"375\",\n",
    "        \"3637\",\n",
    "        \"3272\",\n",
    "        \"153\",\n",
    "    ]\n",
    "candidate_trie = Trie([tokenizer.encode(\"Beauty\") + tokenizer.encode(\"{}\".format(e)) for e in candidates])\n",
    "def prefix_allowed_tokens_fn(candidate_trie):\n",
    "    def prefix_allowed_tokens(batch_id, sentence):\n",
    "        sentence = sentence.tolist()\n",
    "        trie_out = candidate_trie.get(sentence)\n",
    "        return trie_out\n",
    "\n",
    "    return prefix_allowed_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "cff48f4c-4faa-4fb9-9eab-115bf82c7cc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 12587, 2118, 3097, 3328, 1],\n",
       " [0, 12587, 2118, 305, 5062, 1],\n",
       " [0, 12587, 2118, 507, 1828, 1],\n",
       " [0, 12587, 2118, 335, 4056, 1],\n",
       " [0, 12587, 2118, 431, 2079, 1],\n",
       " [0, 12587, 2118, 898, 4591, 1],\n",
       " [0, 12587, 2118, 220, 3891, 1],\n",
       " [0, 12587, 2118, 668, 2555, 1],\n",
       " [0, 12587, 2118, 1902, 2128, 1],\n",
       " [0, 12587, 2118, 1179, 3916, 1],\n",
       " [0, 12587, 2118, 2766, 1],\n",
       " [0, 12587, 2118, 305, 3264, 1],\n",
       " [0, 12587, 2118, 3, 22954, 1],\n",
       " [0, 12587, 2118, 4475, 4118, 1],\n",
       " [0, 12587, 2118, 3538, 5865, 1],\n",
       " [0, 12587, 2118, 3, 27025, 1]]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[0] + tokenizer.encode(\"Beauty item {}\".format(e)) for e in candidates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "687a2083-a348-4880-960e-befebef83aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 4])\n",
      "['<pad> 153</s>', '<pad> 375</s>', '<pad> 599</s>', '<pad> 1398</s>', '<pad> 1825</s>', '<pad> 1683</s>', '<pad> 554</s>', '<pad> 2345</s>', '<pad> 927</s>', '<pad> 363</s>', '<pad> 153</s>', '<pad> 375</s>', '<pad> 599</s>', '<pad> 1825</s>', '<pad> 1398</s>', '<pad> 554</s>', '<pad> 1683</s>', '<pad> 2345</s>', '<pad> 1062</s>', '<pad> 680</s>']\n"
     ]
    }
   ],
   "source": [
    "input_s = [\n",
    "        \"Here is the purchase history of Beauty user 1: Beauty item 1,2,3,4,5. I wonder what is the next recommended item for the user.\",\n",
    "        \"Here is the purchase history of Beauty user 3: Beauty item 27,52,97,2. I wonder what is the next recommended item for the user.\",\n",
    "    ]\n",
    "input_ids = tokenizer.batch_encode_plus(\n",
    "        input_s, padding=\"longest\", return_tensors=\"pt\"\n",
    "    )[\"input_ids\"]\n",
    "prefix_allowed_tokens = prefix_allowed_tokens_fn(candidate_trie)\n",
    "output_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_length=150,\n",
    "        prefix_allowed_tokens_fn=prefix_allowed_tokens,\n",
    "        num_beams=20,\n",
    "        num_return_sequences=10,\n",
    "    )\n",
    "\n",
    "print(output_ids.size())\n",
    "print(tokenizer.batch_decode(output_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d236113c-9586-475c-86eb-17ddbbab2a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {3097: {3328: {1: {}}}, 305: {5062: {1: {}}, 3264: {1: {}}}, 507: {1828: {1: {}}}, 335: {4056: {1: {}}}, 431: {2079: {1: {}}}, 898: {4591: {1: {}}}, 220: {3891: {1: {}}}, 668: {2555: {1: {}}}, 1902: {2128: {1: {}}}, 1179: {3916: {1: {}}}, 2766: {1: {}}, 3: {22954: {1: {}}, 27025: {1: {}}}, 4475: {4118: {1: {}}}, 3538: {5865: {1: {}}}}}\n"
     ]
    }
   ],
   "source": [
    "print(candidate_trie.trie_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d118c817-60ae-420c-bfb1-d4caf0f306d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 3097, 3328, 1],\n",
       " [0, 305, 5062, 1],\n",
       " [0, 507, 1828, 1],\n",
       " [0, 335, 4056, 1],\n",
       " [0, 431, 2079, 1],\n",
       " [0, 898, 4591, 1],\n",
       " [0, 220, 3891, 1],\n",
       " [0, 668, 2555, 1],\n",
       " [0, 1902, 2128, 1],\n",
       " [0, 1179, 3916, 1],\n",
       " [0, 2766, 1],\n",
       " [0, 305, 3264, 1],\n",
       " [0, 3, 22954, 1],\n",
       " [0, 4475, 4118, 1],\n",
       " [0, 3538, 5865, 1],\n",
       " [0, 3, 27025, 1]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[0] + tokenizer.encode(\"{}\".format(e)) for e in candidates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "56e41322-3c4a-4267-a0cc-94badeab9860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3097, 3328, 1]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"3560\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "66e6c47b-d4ab-4212-b984-d4d57f5e5884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[12587, 1, 3097, 3328, 1],\n",
       " [12587, 1, 305, 5062, 1],\n",
       " [12587, 1, 507, 1828, 1],\n",
       " [12587, 1, 335, 4056, 1],\n",
       " [12587, 1, 431, 2079, 1],\n",
       " [12587, 1, 898, 4591, 1],\n",
       " [12587, 1, 220, 3891, 1],\n",
       " [12587, 1, 668, 2555, 1],\n",
       " [12587, 1, 1902, 2128, 1],\n",
       " [12587, 1, 1179, 3916, 1],\n",
       " [12587, 1, 2766, 1],\n",
       " [12587, 1, 305, 3264, 1],\n",
       " [12587, 1, 3, 22954, 1],\n",
       " [12587, 1, 4475, 4118, 1],\n",
       " [12587, 1, 3538, 5865, 1],\n",
       " [12587, 1, 3, 27025, 1]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenizer.encode(\"Beauty\") + tokenizer.encode(\"{}\".format(e)) for e in candidates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c076ab8d-c16a-41df-bd14-375f90e7a998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= [1,2,3,4,5]\n",
    "min(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "838a9773-9431-4c08-a2e6-338ad72e542f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,4]\n",
    "# a[1::4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d5820bbc-5531-4bd7-8182-c2eb55ccd568",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = RandomSampler(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "296848af-96d8-4134-8cd1-1529df364e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "2\n",
      "3\n",
      "1\n",
      "0\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "iterator = sampler.__iter__()\n",
    "for i in range(8):\n",
    "    try:\n",
    "        print(iterator.__next__())\n",
    "    except StopIteration:\n",
    "        iterator = sampler.__iter__()\n",
    "        print(iterator.__next__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c113f62-331b-4709-8d2a-7f57e7bcd1c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SP5",
   "language": "python",
   "name": "sp5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
